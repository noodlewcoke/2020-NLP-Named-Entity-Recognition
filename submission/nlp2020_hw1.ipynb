{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp2020-hw1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX4_F4gEjqpD",
        "colab_type": "text"
      },
      "source": [
        "**LOAD LIBRARIES**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukn2-kzi0-0O",
        "colab_type": "code",
        "outputId": "b262eb51-b4be-4dbc-bec4-a77e5a311cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_score as sk_precision\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "main_path = '/content/drive/My Drive/Colab Notebooks/nlp2020-hw1'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_CTgHSdcPuN",
        "colab_type": "text"
      },
      "source": [
        "**DATASET HANDLER**\n",
        "\n",
        "Run every time. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7XppHrKTtLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocabularize(paths, save_path=None):\n",
        "    '''\n",
        "    # This function creates a vocabulary containing both cased and lowercased words\n",
        "    # The purpose of this is to train NER model with mixture of cased and lowercased sentences \n",
        "    # which was proved to work best on average for cased and lowercased datasets\n",
        "    '''\n",
        "\n",
        "    characters = ''\n",
        "    words = []\n",
        "\n",
        "    #* Word vocabulary    \n",
        "    for path in paths:\n",
        "        with open(path, 'r') as f:\n",
        "            for sentence in f:\n",
        "                sentence = sentence.strip()\n",
        "                if sentence.startswith('#'):\n",
        "                    lower_sentence = sentence.lower()\n",
        "                    lower_sentence = lower_sentence.split()\n",
        "                    sentence = sentence.split()\n",
        "                    unique_words = list(set(sentence[1:]+lower_sentence[1:]))\n",
        "                    words.extend(unique_words)\n",
        "            f.close()\n",
        "\n",
        "    words = sorted(list(set(words)))\n",
        "    words.insert(0, '<UNK>')\n",
        "    words.insert(0, '<PAD>')\n",
        "    wordtoindex = {v: k for k, v in enumerate(words)}\n",
        "    indextoword = {k: v for k, v in enumerate(words)}\n",
        "    vocab = {'w2i' : wordtoindex,\n",
        "             'i2w' : indextoword}\n",
        "\n",
        "    #* Char vocabulary\n",
        "    for path in paths:\n",
        "            with open(path, 'r') as f:\n",
        "                reader = f.read()\n",
        "                reader += reader.lower()\n",
        "                characters += reader\n",
        "                f.close()\n",
        "    characters = sorted(list(set(characters)))\n",
        "    characters.remove('\\n')\n",
        "    characters.remove('\\t')\n",
        "    characters.insert(0, '<UNK>')\n",
        "    characters.insert(0, '<PAD>')\n",
        "\n",
        "    chartoindex = {v: k for k, v in enumerate(characters)}\n",
        "    indextochar = {k: v for k, v in enumerate(characters)}\n",
        "    char_vocab = {  'c2i' : chartoindex,\n",
        "                    'i2c' : indextochar}\n",
        "\n",
        "    print(\"Word vocabulary size: \", len(words))\n",
        "    print(\"Char vocabulary size: \", len(characters))\n",
        "    if save_path:\n",
        "        np.save(save_path+'wordsList', words)\n",
        "        with open(save_path+'word_vocab.json', 'w') as f:\n",
        "            json.dump(vocab, f)\n",
        "            f.close()\n",
        "        np.save(save_path+'charList', characters)\n",
        "        with open(save_path+'char_vocab.json', 'w') as f:\n",
        "            json.dump(char_vocab, f)\n",
        "            f.close()\n",
        "    return char_vocab, vocab, characters, words\n",
        "\n",
        "\n",
        "class DataSetHandler:\n",
        "    '''\n",
        "    A class for building a dataset out of provided data, given also vocabularies for words and characters.\n",
        "    '''\n",
        "    def __init__(self, \n",
        "                batch_size, \n",
        "                word_vocabulary, \n",
        "                char_vocabulary,\n",
        "                ignore_singletons = False):\n",
        "        self.batch_size = batch_size\n",
        "        self.word_vocabulary = word_vocabulary['w2i']\n",
        "        self.char_vocabulary = char_vocabulary['c2i']   \n",
        "\n",
        "        # Flags that check what type of dataset is stored. str format or int format (vocabulary index)     \n",
        "        self.enumerated = False\n",
        "        self.str_dataset = False\n",
        "\n",
        "        self.counter = 0\n",
        "\n",
        "        # A flag for randomly turning words occuring once in the data set to the <UNK> tag.\n",
        "        self.ignore_singletons = ignore_singletons\n",
        "\n",
        "    def load_dataset(self, file_path, shuffle=False):\n",
        "        '''\n",
        "        This function loads str format dataset\n",
        "        '''\n",
        "        self.dataset = np.load(file_path, allow_pickle=True)\n",
        "        self.data_len = len(self.dataset)\n",
        "        self.str_dataset = True\n",
        "        if shuffle: np.random.shuffle(self.dataset)\n",
        "        print('Dataset in {} loaded. Number of samples: {}'.format(file_path, self.data_len))\n",
        "\n",
        "    def load_enumerated_dataset(self, file_path, shuffle=False):\n",
        "        '''\n",
        "        This function loads int format dataset\n",
        "        '''\n",
        "        self.enum_dataset = np.load(file_path, allow_pickle=True)\n",
        "        self.data_len = len(self.enum_dataset)\n",
        "        self.enumerated = True\n",
        "        if shuffle: np.random.shuffle(self.enum_dataset)\n",
        "        print('Enumerated dataset in {} loaded. Number of samples: {}'.format(file_path, self.data_len))\n",
        "\n",
        "\n",
        "    def create_dataset(self, file_path, mode = 'true', shuffle=False):\n",
        "        '''\n",
        "        Creates str format dataset.\n",
        "        mode: Stands for capitalization mode. Either 'true' for true case, 'lower' for lower case and 'mixed' for both true and lower case together.\n",
        "        '''\n",
        "\n",
        "        self.mode = mode.lower()\n",
        "        assert self.mode == 'true' or self.mode == 'lower' or self.mode == 'mixed', \"Mode variable, denoting casing, should either be 'true', 'lower' or 'mixed'.\"\n",
        "\n",
        "        with open(file_path, 'r') as tsv:\n",
        "            sentences = tsv.read().split('#')[1:]\n",
        "            tsv.close()\n",
        "\n",
        "        self.dataset = []\n",
        "        for data in sentences:\n",
        "            data = data.strip().split('\\n')\n",
        "            assert len(data[0].split()) == len(data[1:])\n",
        "\n",
        "            tokens, chars, labels = [], [], []\n",
        "            lower_tokens, lower_chars = [], []\n",
        "            for line in data[1:]:\n",
        "                _, token, label = line.split('\\t')\n",
        "\n",
        "                #* Case mode handling\n",
        "                if self.mode == 'lower':\n",
        "                    token = token.lower()\n",
        "                elif self.mode == 'mixed':\n",
        "                    lower_token = token.lower()\n",
        "                    lower_tokens.append(lower_token)\n",
        "                    lower_chars.append(list(lower_token))\n",
        "\n",
        "                tokens.append(token)\n",
        "                chars.append(list(token))\n",
        "\n",
        "                #* Label handling : ORG -> 1, PER -> 2, LOC -> 3, O -> 4\n",
        "                if label == 'ORG':\n",
        "                    label = 1\n",
        "                elif label == 'PER':\n",
        "                    label = 2\n",
        "                elif label == 'LOC':\n",
        "                    label = 3\n",
        "                elif label == 'O':\n",
        "                    label = 4\n",
        "                else:\n",
        "                    print(\"Problem with this label: \", label)\n",
        "                labels.append(label)\n",
        "\n",
        "            self.dataset.append((tokens, chars, labels))\n",
        "            if self.mode == 'mixed':\n",
        "                self.dataset.append((lower_tokens, lower_chars, labels))\n",
        "        del sentences\n",
        "        print('Dataset has been created.')\n",
        "\n",
        "        self.data_len = len(self.dataset)\n",
        "        self.str_dataset = True\n",
        "        if shuffle: np.random.shuffle(self.dataset)\n",
        "        if self.ignore_singletons: self.tokenfreq()\n",
        "\n",
        "    def enumerate(self, data):\n",
        "        '''\n",
        "        Enumeration stands for turning the str formatted data into int format showing indices in the vocabulary.\n",
        "        '''\n",
        "\n",
        "        enum_data = []\n",
        "        for sentence, chars, labels in tqdm(data):\n",
        "            enum_sentence = []\n",
        "            enum_chars = []\n",
        "            for word, cword in zip(sentence, chars):\n",
        "                # Word enumeration\n",
        "                if word in self.wordkeys:\n",
        "                    if self.ignore_singletons and word in self.least_frequent and np.random.rand(1)[0] < 0.5:\n",
        "                        '''\n",
        "                        This replaces tokens with appearance less than one, with the <UNK> tag, with probability 0.5 . \n",
        "                        '''\n",
        "                        enum_sentence.append(self.word_vocabulary['<UNK>'])\n",
        "                    else:\n",
        "                        enum_sentence.append(self.word_vocabulary[word])\n",
        "                else:\n",
        "                    enum_sentence.append(self.word_vocabulary['<UNK>'])\n",
        "                enum_char = []\n",
        "\n",
        "                # Character enumeration\n",
        "                for char in cword:\n",
        "                    if char in self.charkeys:\n",
        "                        enum_char.append(self.char_vocabulary[char])\n",
        "                    else:\n",
        "                        enum_char.append(self.char_vocabulary['<UNK>'])\n",
        "                enum_chars.append(enum_char)\n",
        "            enum_data.append((enum_sentence, enum_chars, labels))\n",
        "        return enum_data\n",
        "\n",
        "\n",
        "    def token2index(self):\n",
        "        '''\n",
        "        Parallelly run four processes that enumerate the dataset.\n",
        "        '''\n",
        "\n",
        "        print('Enumerating the dataset.')\n",
        "        \n",
        "        self.enum_dataset = []\n",
        "        self.wordkeys = list(self.word_vocabulary.keys())\n",
        "        self.charkeys = list(self.char_vocabulary.keys())\n",
        "        lenght = len(self.dataset)\n",
        "\n",
        "        # This could have been automatized for an arbitrary number of processes but for the time of making this is how I went with it.\n",
        "        data1 = self.dataset[:lenght//4]\n",
        "        data2 = self.dataset[lenght//4 : lenght//2]\n",
        "        data3 = self.dataset[lenght//2 : 3*lenght//4]\n",
        "        data4 = self.dataset[3*lenght//4 :]\n",
        "        results = []\n",
        "        with Pool(4) as p:\n",
        "            results = p.map(self.enumerate, [data1, data2, data3, data4])\n",
        "        self.enum_dataset = [sample for data in results for sample in data]\n",
        "    \n",
        "        assert len(self.dataset) == len(self.enum_dataset), \"Mistakes were made. Lenghts of the datasets don't match.\"\n",
        "        self.enumerated = True\n",
        "        print(\"Dataset has been enumerated.\")\n",
        "        return self.enum_dataset\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        '''\n",
        "        Data set must be enumerated for this to work. \n",
        "        Takes a batch of data points, padds them and turns them into torch Tensors.\n",
        "        Returns a batch, with the size spesified in the class constructor.\n",
        "        '''\n",
        "\n",
        "        if self.counter < self.data_len and self.enumerated:\n",
        "            last = min(self.counter+self.batch_size, self.data_len)\n",
        "            batch = self.enum_dataset[self.counter:last]\n",
        "\n",
        "            # Sort for pack_padded_sequence funtion in the BiLSTM module.\n",
        "            batch = np.array(sorted(batch, key=lambda d: len(d[0]), reverse=True))\n",
        "            data_w = batch[:, 0] # List of tokens\n",
        "            data_c = batch[:, 1] # List of list of characters\n",
        "            data_y = batch[:, 2] # List of labels\n",
        "            sentence_lengths = [len(i) for i in data_w]\n",
        "\n",
        "            data_w = [torch.LongTensor(sentence) for sentence in data_w] \n",
        "            data_c = [[torch.LongTensor(word) for word in sentence] for sentence in data_c]\n",
        "            y = [torch.LongTensor(i) for i in data_y]\n",
        "\n",
        "            padded_w = pad_sequence(data_w, batch_first=True, padding_value=self.word_vocabulary['<PAD>'])\n",
        "            padded_y = pad_sequence(y, batch_first=True, padding_value=0)\n",
        "            self.counter = last\n",
        "            return padded_w, data_c, padded_y, sentence_lengths\n",
        "        else:\n",
        "            if not self.enumerated:\n",
        "                raise ValueError('Dataset must be enumerated first. Use the token2index() funtion.')\n",
        "            self.counter = 0\n",
        "            raise StopIteration()\n",
        "\n",
        "    def save(self, save_path):\n",
        "        if self.str_dataset:\n",
        "            np.save(save_path, self.dataset, allow_pickle=True)\n",
        "            print('Dataset has been saved in {}'.format(save_path))\n",
        "        if self.enumerated:\n",
        "            np.save(save_path+'_enumerated', self.enum_dataset, allow_pickle=True)\n",
        "            print('Enumerated dataset has been saved in {}'.format(save_path+'_enumerated'))\n",
        "        if not self.str_dataset and not self.enumerated:\n",
        "            raise ValueError('There are no datasets to save.')\n",
        "\n",
        "    def tokenfreq(self):\n",
        "        '''\n",
        "        Use this function only with the training set!\n",
        "        Compute the frequency of each token.\n",
        "        This is utilized later to replace least frequent tokens with the <UNK> tag.\n",
        "        '''\n",
        "\n",
        "        assert self.str_dataset, 'Dataset should be in string format.'\n",
        "\n",
        "        self.token_fq = {k: 0 for k in self.word_vocabulary.keys()}\n",
        "        for sentence, _, _ in self.dataset:\n",
        "            for token in sentence:\n",
        "                try:\n",
        "                    self.token_fq[token] += 1\n",
        "                except:\n",
        "                    print(\"Unexpected token: \", token)\n",
        "\n",
        "        aslist = sorted(list(self.token_fq.items()), key = lambda x: x[1])\n",
        "        self.least_frequent = []\n",
        "        n_replaced = 0\n",
        "        for t, f in aslist:\n",
        "            if f < 2:\n",
        "                self.least_frequent.append(t)\n",
        "        self.ignore_singletons = True\n",
        "\n",
        "        return self.token_fq, self.least_frequent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL6NUaygcX02",
        "colab_type": "text"
      },
      "source": [
        "**BUILD VOCABULARY**\n",
        "\n",
        "Build once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh_nl15DT0BW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainFileName = main_path+'/data/train.tsv'\n",
        "\n",
        "vocabularize([trainFileName], main_path+'/data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FsN3Q5DoH0h",
        "colab_type": "text"
      },
      "source": [
        "**BUILD DATA SETS**\n",
        "\n",
        "Build once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVNn_FbmVQVz",
        "colab_type": "code",
        "outputId": "f027cc09-659d-40d9-9fab-0ce879ca9e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "trainFileName = main_path + '/data/' + 'train.tsv'\n",
        "devFileName = main_path + '/data/' + 'dev.tsv'\n",
        "testFileName = main_path + '/data/' + 'test.tsv'\n",
        "\n",
        "with open(main_path + '/data/word_vocab.json', 'r') as f:\n",
        "    word_vocab = json.load(f)\n",
        "    f.close()\n",
        "with open(main_path + '/data/char_vocab.json', 'r') as f:\n",
        "    char_vocab = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "trainset = DataSetHandler(32, word_vocab, char_vocab, ignore_singletons=True)\n",
        "trainset.create_dataset(trainFileName, mode='true', shuffle=True)\n",
        "trainset.token2index(num_workers = 5)\n",
        "trainset.save(main_path + '/data/trainset_truecase_unk')\n",
        "\n",
        "devset = DataSetHandler(32, word_vocab, char_vocab)\n",
        "devset.create_dataset(devFileName, mode='true', shuffle=False)\n",
        "devset.token2index(num_workers = 5)\n",
        "devset.save(main_path + '/data/devset_truecase')\n",
        "\n",
        "testset = DataSetHandler(32, word_vocab, char_vocab)\n",
        "testset.create_dataset(testFileName, mode='true', shuffle=False)\n",
        "testset.token2index(num_workers = 5)\n",
        "testset.save(main_path + '/data/testset_truecase')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset has been created.\n",
            "Enumerating the dataset.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 25000/25000 [1:26:39<00:00,  4.81it/s]\n",
            "100%|██████████| 25000/25000 [1:28:04<00:00,  4.73it/s]\n",
            "100%|██████████| 25000/25000 [1:28:30<00:00,  4.71it/s]\n",
            "100%|██████████| 25000/25000 [1:28:28<00:00,  4.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset has been enumerated.\n",
            "Dataset has been saved in /content/drive/My Drive/Colab Notebooks/nlp2020-hw1/data/trainset_truecase_unk\n",
            "Enumerated dataset has been saved in /content/drive/My Drive/Colab Notebooks/nlp2020-hw1/data/trainset_truecase_unk_enumerated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCsW20-ydK9S",
        "colab_type": "text"
      },
      "source": [
        "**CREATE MODEL**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bE8w0ReXSyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class nerBiLSTM(torch.nn.Module):\n",
        "    def __init__(self, vocab_dim, embedding_dim, hidden_dim, output_dim, lr=1e-3):\n",
        "        super(nerBiLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding_layer = torch.nn.Embedding(vocab_dim, embedding_dim, padding_idx=0)\n",
        "        self.bilstm = torch.nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\n",
        "        self.fc = torch.nn.Linear(2*hidden_dim, output_dim)\n",
        "\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, sentence, X_lengths):\n",
        "        x = sentence \n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
        "\n",
        "        x, self.hidden = self.bilstm(x)        \n",
        "\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        self.logits = self.fc(x)\n",
        "\n",
        "        self.output = F.softmax(self.logits, dim=2)\n",
        "        return self.logits, self.output\n",
        "\n",
        "    def update(self, sentence, label, X_lengths):\n",
        "        output, _ = self(sentence, X_lengths)\n",
        "\n",
        "        output = output.view(-1, self.output_dim)\n",
        "\n",
        "        label = label.contiguous()\n",
        "        label = label.view(-1)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.loss_function(output, label)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, sentence, label, X_lengths):\n",
        "        with torch.no_grad():\n",
        "            output, _ = self(sentence, X_lengths)\n",
        "\n",
        "            output = output.view(-1, self.output_dim)\n",
        "\n",
        "            label = label.contiguous()\n",
        "            label = label.view(-1)\n",
        "\n",
        "            loss = self.loss_function(output, label)\n",
        "        return loss\n",
        "\n",
        "    def prediction(self, sentence, X_lengths):\n",
        "        with torch.no_grad():\n",
        "            _, output = self(sentence, X_lengths)\n",
        "            return output[:, :, 1:]\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save(self.state_dict(), filepath)\n",
        "\n",
        "    def load(self, filepath, device):\n",
        "        self.load_state_dict(torch.load(filepath, map_location=device))\n",
        "\n",
        "\n",
        "#BEWARE : EXPERIMENTAL MODEL\n",
        "class nerBiLSTM_EXP(torch.nn.Module):\n",
        "    def __init__(self, vocab_dim, embedding_dim, hidden_dim, output_dim, lr=1e-3, lcf_dim = 1):\n",
        "        super(nerBiLSTM_EXP, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.lcf_dim = lcf_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding_layer = torch.nn.Embedding(vocab_dim, embedding_dim, padding_idx=0)\n",
        "        self.bilstm = torch.nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "  \n",
        "        self.fc = torch.nn.Linear(2*hidden_dim + output_dim -1 , output_dim)\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, sentence, X_lengths):\n",
        "        x = sentence \n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
        "\n",
        "        x, self.hidden = self.bilstm(x)        \n",
        "\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        x = x.transpose(0, 1)\n",
        "        self.logits = []\n",
        "        self.output = []\n",
        "\n",
        "        prev_label = torch.zeros((x.size()[1], self.output_dim-1)).to(x.device)\n",
        "\n",
        "        # 'Condition' each instance on the previous label\n",
        "        for sentence in x:\n",
        "            exp = torch.cat((sentence, prev_label), -1)\n",
        "            exp = self.fc(exp)\n",
        "            self.logits.append(exp)\n",
        "            exp = F.softmax(exp[:,1:], dim=-1)\n",
        "            self.output.append(exp)\n",
        "            prev_label = exp\n",
        "        self.logits = torch.stack(self.logits, 0).transpose(0,1)\n",
        "        self.output = torch.stack(self.output, 0).transpose(0,1)\n",
        "\n",
        "        return self.logits, self.output\n",
        "\n",
        "    def update(self, sentence, label, X_lengths):\n",
        "        output, _ = self(sentence, X_lengths)\n",
        "        output = output.contiguous()\n",
        "        output = output.view(-1, self.output_dim)\n",
        "\n",
        "        label = label.contiguous()\n",
        "        label = label.view(-1)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.loss_function(output, label)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, sentence, label, X_lengths):\n",
        "        with torch.no_grad():\n",
        "            output, _ = self(sentence, X_lengths)\n",
        "            output = output.contiguous()\n",
        "\n",
        "            output = output.view(-1, self.output_dim)\n",
        "\n",
        "            label = label.contiguous()\n",
        "            label = label.view(-1)\n",
        "\n",
        "            loss = self.loss_function(output, label)\n",
        "        return loss\n",
        "\n",
        "    def prediction(self, sentence, X_lengths):\n",
        "        with torch.no_grad():\n",
        "            _, output = self(sentence, X_lengths)\n",
        "            return output\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save(self.state_dict(), filepath)\n",
        "\n",
        "    def load(self, filepath, device):\n",
        "        self.load_state_dict(torch.load(filepath, map_location=device))\n",
        "\n",
        "\n",
        "class nerBiLSTM_char(torch.nn.Module):\n",
        "    '''\n",
        "    BiLSTM with char embeddings and word embeddings. Like described in the reference paper.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, \n",
        "                vocab_dim, \n",
        "                embedding_dim, \n",
        "                hidden_dim, \n",
        "                output_dim,\n",
        "                char_vocab_dim, \n",
        "                char_embedding_dim,  \n",
        "                char_hidden_dim,\n",
        "                lr=1e-3,\n",
        "                d_p=0.5):\n",
        "        super(nerBiLSTM_char, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.char_hidden_dim = char_hidden_dim\n",
        "\n",
        "        self.char_embedding_layer = torch.nn.Embedding(char_vocab_dim, char_embedding_dim, padding_idx = 0)\n",
        "        self.char_f_lstm = torch.nn.LSTM(char_embedding_dim, char_hidden_dim, batch_first=True, bidirectional=False)\n",
        "        self.char_b_lstm = torch.nn.LSTM(char_embedding_dim, char_hidden_dim, batch_first=True, bidirectional=False)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(d_p)\n",
        "        self.embedding_layer = torch.nn.Embedding(vocab_dim, embedding_dim, padding_idx=0)\n",
        "        self.bilstm = torch.nn.LSTM(embedding_dim + char_hidden_dim*2, hidden_dim, bidirectional=True)\n",
        "\n",
        "        self.fc = torch.nn.Linear(2*hidden_dim, output_dim)\n",
        "\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def character_embedding(self, sentences, max_len, device):\n",
        "        embedded_sentence = []\n",
        "        for sentence_char in sentences:\n",
        "            lengths = np.array(list(map(len, sentence_char)))-1\n",
        "            padded_tokens = pad_sequence(sentence_char, batch_first=True, padding_value=0)\n",
        "            w = self.char_embedding_layer(padded_tokens.to(device))\n",
        "            f = self.char_f_lstm(w)[0]\n",
        "            f = f[np.arange(len(lengths)), lengths]\n",
        "            b = self.char_b_lstm(w)[0][:, -1, :]\n",
        "            w = torch.cat([f,b], dim = -1)\n",
        "            n_words = len(w)\n",
        "            pad = torch.zeros([max_len-n_words, w.size()[1]]).to(device)\n",
        "            w = torch.cat([w, pad], dim=0)\n",
        "            embedded_sentence.append(w)\n",
        "        embedded_sentence = torch.stack(embedded_sentence, dim=0)\n",
        "\n",
        "        return embedded_sentence\n",
        "\n",
        "    def forward(self, sentences, characters, X_lengths):\n",
        "        x = sentences\n",
        "        x = self.embedding_layer(x)\n",
        "        device = x.device\n",
        "        chars = self.character_embedding(characters, int(torch.max(X_lengths).item()), device)\n",
        "        \n",
        "        x = torch.cat([x, chars], -1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
        "\n",
        "        x, self.hidden = self.bilstm(x)        \n",
        "\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "        self.logits = self.fc(x)\n",
        "        self.output = F.softmax(self.logits, dim=2)\n",
        "        return self.logits, self.output\n",
        "\n",
        "    def update(self, sentence, characters, label, X_lengths):\n",
        "        output, _ = self(sentence, characters, X_lengths)\n",
        "\n",
        "        output = output.view(-1, self.output_dim)\n",
        "\n",
        "        label = label.contiguous()\n",
        "        label = label.view(-1)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.loss_function(output, label)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, sentence, characters, label, X_lengths):\n",
        "        with torch.no_grad():\n",
        "            output, _ = self(sentence, characters, X_lengths)\n",
        "\n",
        "            output = output.view(-1, self.output_dim)\n",
        "\n",
        "            label = label.contiguous()\n",
        "            label = label.view(-1)\n",
        "\n",
        "            loss = self.loss_function(output, label)\n",
        "        return loss\n",
        "\n",
        "    def prediction(self, sentence, characters, X_lengths):\n",
        "        with torch.no_grad():\n",
        "            _, output = self(sentence, characters, X_lengths)\n",
        "            return output[:, :, 1:]\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save(self.state_dict(), filepath)\n",
        "\n",
        "    def load(self, filepath, device):\n",
        "        self.load_state_dict(torch.load(filepath, map_location=device))\n",
        "\n",
        "\n",
        "class nerBiLSTM_ULT(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                vocab_dim, \n",
        "                embedding_dim, \n",
        "                hidden_dim, \n",
        "                output_dim,\n",
        "                char_vocab_dim, \n",
        "                char_embedding_dim,  \n",
        "                char_hidden_dim,\n",
        "                lr=1e-3,\n",
        "                d_p=0.5):\n",
        "        super(nerBiLSTM_ULT, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.char_hidden_dim = char_hidden_dim\n",
        "\n",
        "        self.char_embedding_layer = torch.nn.Embedding(char_vocab_dim, char_embedding_dim, padding_idx = 0)\n",
        "        self.char_f_lstm = torch.nn.LSTM(char_embedding_dim, char_hidden_dim, batch_first=True, bidirectional=False)\n",
        "        self.char_b_lstm = torch.nn.LSTM(char_embedding_dim, char_hidden_dim, batch_first=True, bidirectional=False)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(d_p)\n",
        "        self.embedding_layer = torch.nn.Embedding(vocab_dim, embedding_dim, padding_idx=0)\n",
        "        self.bilstm = torch.nn.LSTM(embedding_dim + char_hidden_dim*2, hidden_dim, bidirectional=True)\n",
        "  \n",
        "        self.fc = torch.nn.Linear(2*hidden_dim + output_dim -1 , output_dim)\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def character_embedding(self, sentences, max_len, device):\n",
        "        embedded_sentence = []\n",
        "        for sentence_char in sentences:\n",
        "            lengths = np.array(list(map(len, sentence_char)))-1\n",
        "            padded_tokens = pad_sequence(sentence_char, batch_first=True, padding_value=0)\n",
        "            w = self.char_embedding_layer(padded_tokens.to(device))\n",
        "            f = self.char_f_lstm(w)[0]\n",
        "            f = f[np.arange(len(lengths)), lengths]\n",
        "            b = self.char_b_lstm(w)[0][:, -1, :]\n",
        "            w = torch.cat([f,b], dim = -1)\n",
        "            n_words = len(w)\n",
        "            pad = torch.zeros([max_len-n_words, w.size()[1]]).to(device)\n",
        "            w = torch.cat([w, pad], dim=0)\n",
        "            embedded_sentence.append(w)\n",
        "        embedded_sentence = torch.stack(embedded_sentence, dim=0)\n",
        "\n",
        "        return embedded_sentence\n",
        "\n",
        "    def forward(self, sentences, characters, X_lengths):\n",
        "        x = sentences\n",
        "        x = self.embedding_layer(x)\n",
        "        device = x.device\n",
        "        chars = self.character_embedding(characters, int(torch.max(X_lengths).item()), device)\n",
        "        \n",
        "        x = torch.cat([x, chars], -1)\n",
        "        x = self.dropout(x)\n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
        "\n",
        "        x, self.hidden = self.bilstm(x)        \n",
        "\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        x = x.transpose(0, 1)\n",
        "        self.logits = []\n",
        "        self.output = []\n",
        "\n",
        "        prev_label = torch.zeros((x.size()[1], self.output_dim-1)).to(x.device)\n",
        "\n",
        "        # 'Condition' each instance on the previous label\n",
        "        for sentence in x:\n",
        "            exp = torch.cat((sentence, prev_label), -1)\n",
        "            exp = self.fc(exp)\n",
        "            self.logits.append(exp)\n",
        "            exp = F.softmax(exp[:,1:], dim=-1)\n",
        "            self.output.append(exp)\n",
        "            prev_label = exp\n",
        "        self.logits = torch.stack(self.logits, 0).transpose(0,1)\n",
        "        self.output = torch.stack(self.output, 0).transpose(0,1)\n",
        "\n",
        "        return self.logits, self.output\n",
        "\n",
        "    def update(self, sentence, characters, label, X_lengths):\n",
        "        output, _ = self(sentence, characters, X_lengths)\n",
        "        output = output.contiguous()\n",
        "        output = output.view(-1, self.output_dim)\n",
        "\n",
        "        label = label.contiguous()\n",
        "        label = label.view(-1)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.loss_function(output, label)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, sentence, characters, label, X_lengths):\n",
        "        with torch.no_grad():\n",
        "            output, _ = self(sentence, characters, X_lengths)\n",
        "            output = output.contiguous()\n",
        "\n",
        "            output = output.view(-1, self.output_dim)\n",
        "\n",
        "            label = label.contiguous()\n",
        "            label = label.view(-1)\n",
        "\n",
        "            loss = self.loss_function(output, label)\n",
        "        return loss\n",
        "\n",
        "    def prediction(self, sentence, characters, X_lengths):\n",
        "        with torch.no_grad():\n",
        "            _, output = self(sentence, characters, X_lengths)\n",
        "            return output\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save(self.state_dict(), filepath)\n",
        "\n",
        "    def load(self, filepath, device):\n",
        "        self.load_state_dict(torch.load(filepath, map_location=device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwizhuZWd8Pw",
        "colab_type": "text"
      },
      "source": [
        "**HYPERPARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TpnaZcad_D-",
        "colab_type": "code",
        "outputId": "ada9e01b-e3d0-4213-b231-321b8384cf5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_batch_size = 32\n",
        "dev_batch_size = 128\n",
        "test_batch_size = 128\n",
        "EPOCHS = 25\n",
        "EXPERIMENT = '10a'\n",
        "\n",
        "save_path = main_path + '/experiments/experiment' + EXPERIMENT\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboZrQidb-QP",
        "colab_type": "text"
      },
      "source": [
        "**LOAD DATASETS and VOCABULARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiMHNoY2WzcX",
        "colab_type": "code",
        "outputId": "c4947b62-9ed3-4beb-e688-726277013911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_path = main_path + '/data/trainset_truecase_unk_enumerated.npy'\n",
        "dev_path = main_path + '/data/devset_enumerated.npy'\n",
        "test_path = main_path + '/data/testset_enumerated.npy'\n",
        "with open(main_path + '/data/word_vocab.json', 'r') as f:\n",
        "    word_vocab = json.load(f)\n",
        "    f.close()\n",
        "with open(main_path + '/data/char_vocab.json', 'r') as f:\n",
        "    char_vocab = json.load(f)\n",
        "    f.close()\n",
        "\n",
        "trainset = DataSetHandler(train_batch_size, word_vocab, char_vocab)\n",
        "trainset.load_enumerated_dataset(train_path, shuffle=True)\n",
        "\n",
        "devset = DataSetHandler(dev_batch_size, word_vocab, char_vocab)\n",
        "devset.load_enumerated_dataset(dev_path, shuffle=False)\n",
        "\n",
        "testset = DataSetHandler(test_batch_size, word_vocab, char_vocab)\n",
        "testset.load_enumerated_dataset(test_path, shuffle=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enumerated dataset in /content/drive/My Drive/Colab Notebooks/nlp2020-hw1/data/trainset_truecase_unk_enumerated.npy loaded. Number of samples: 100000\n",
            "Enumerated dataset in /content/drive/My Drive/Colab Notebooks/nlp2020-hw1/data/devset_enumerated.npy loaded. Number of samples: 14434\n",
            "Enumerated dataset in /content/drive/My Drive/Colab Notebooks/nlp2020-hw1/data/testset_enumerated.npy loaded. Number of samples: 15474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_ADvnsb6Ad",
        "colab_type": "text"
      },
      "source": [
        "**BUILD** **MODEL**\n",
        "\n",
        "Build only the one you want to train or test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQcQTxLjpQQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_dim = len(word_vocab['w2i'])\n",
        "model = nerBiLSTM(vocab_dim, 100, 50, 5).to(device)\n",
        "# model.load(save_path+'/model', device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpfHa2o6bzxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_dim = len(word_vocab['w2i'])\n",
        "model = nerBiLSTM_EXP(vocab_dim, 100, 50, 5).to(device)\n",
        "# model.load(save_path+'/model', device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMxFVIf2tAks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_dim = len(word_vocab['w2i'])\n",
        "char_dim = len(char_vocab['c2i'])\n",
        "model = nerBiLSTM_char(vocab_dim, 100, 50, 5, char_dim, 50, 25, d_p=0.0).to(device)\n",
        "# model.load(save_path+'/model15', device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6HmEhTKWSpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_dim = len(word_vocab['w2i'])\n",
        "char_dim = len(char_vocab['c2i'])\n",
        "model = nerBiLSTM_ULT(vocab_dim, 100, 50, 5, char_dim, 50, 25, d_p=0.0).to(device)\n",
        "# model.load(save_path+'/model25', device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7lQ6-8fdX4g",
        "colab_type": "text"
      },
      "source": [
        "**TRAINING**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1nVEykrdaAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_losses = []\n",
        "training_epoch_losses = []\n",
        "dev_losses = []\n",
        "test_accuracy = []\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    episode = 1\n",
        "    print('Training..')\n",
        "    print('+=========================================+\\n')\n",
        "    for words, chars, label, lengths in trainset:\n",
        "        # loss = model.update(words.to(device), label.to(device), torch.Tensor(lengths).to(device)) # For BiLSTM or BiLSTM_EXP\n",
        "        loss = model.update(words.to(device), chars, label.to(device), torch.Tensor(lengths).to(device)) # For BiLSTM_char\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if not episode % 400:\n",
        "            print('Epoch: {:2d}\\tEpisode: {:3d}\\tAvg Loss: {:4.6f}'.format(epoch, episode, epoch_loss/episode))\n",
        "            training_losses.append(epoch_loss/episode)\n",
        "        episode += 1\n",
        "    print('Epoch: {:2d}\\tTraining Loss: {:4.6f}'.format(epoch, epoch_loss/episode))\n",
        "    training_epoch_losses.append(epoch_loss/episode)\n",
        "\n",
        "    print('Validation.. ')\n",
        "    print('+=========================================+\\n')\n",
        "    episode = 1\n",
        "    epoch_loss = 0\n",
        "    for words, chars, label, lengths in devset:\n",
        "        # loss = model.evaluate(words.to(device), label.to(device), torch.Tensor(lengths).to(device)) # For BiLSTM or BiLSTM_EXP\n",
        "        loss = model.evaluate(words.to(device), chars, label.to(device), torch.Tensor(lengths).to(device)) # For BiLSTM_char\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        episode += 1\n",
        "    print('Epoch: {:2d}\\tValidation Loss: {:4.6f}'.format(epoch, epoch_loss/episode))\n",
        "    dev_losses.append(epoch_loss/episode)\n",
        "\n",
        "    model.save(save_path + '/model{}'.format(epoch))\n",
        "    np.save(save_path + '/training_losses', training_losses)\n",
        "    np.save(save_path + '/training_epoch_losses', training_epoch_losses)\n",
        "    np.save(save_path + '/dev_losses', dev_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSxCpvBlb1hK",
        "colab_type": "text"
      },
      "source": [
        "**TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbLg456V558s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model selection\n",
        "dev_losses = np.load(save_path+'/dev_losses.npy', allow_pickle=True)\n",
        "training_epoch_losses = np.load(save_path+'/training_epoch_losses.npy', allow_pickle=True)\n",
        "training_losses = np.load(save_path+'/training_losses.npy', allow_pickle=True)\n",
        "for t, (i, j) in enumerate(zip(training_epoch_losses, dev_losses)):\n",
        "  print(t+1, i, j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpKYYWa9XGXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "442e0d80-b456-4657-becd-12d38a9e76a3"
      },
      "source": [
        "test_accuracy = []\n",
        "n_model = '26'\n",
        "# for n_model in range(26,31):\n",
        "predictions, labels = [], []\n",
        "model.load(save_path+'/model{}'.format(n_model), device)\n",
        "\n",
        "for w, c, y, l in testset:\n",
        "    # prediction = model.prediction(w.cuda(), torch.Tensor(l).cuda()) # For BiLSTM or BiLSTM_EXP\n",
        "    prediction = model.prediction(w.cuda(), c, torch.Tensor(l).cuda()) # For BiLSTM_char\n",
        "    pred = torch.argmax(prediction, dim=2)\n",
        "    pred += torch.ones_like(pred)\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(pred, l, batch_first=True)\n",
        "    y = torch.nn.utils.rnn.pack_padded_sequence(y, l, batch_first=True)\n",
        "    predictions.extend(x.data.tolist())\n",
        "    labels.extend(y.data.tolist())\n",
        "\n",
        "precision_micro = sk_precision(labels, predictions, average='micro')\n",
        "precision_macro = sk_precision(labels, predictions, average='macro')\n",
        "f1 = f1_score(labels, predictions, average='macro')\n",
        "con_matrix = confusion_matrix(labels, predictions)\n",
        "print(\"Model\", n_model)\n",
        "print(\"Precision Micro\", precision_micro)\n",
        "print(\"Precision Macro\", precision_macro)\n",
        "print(\"F1 Score       \", f1)\n",
        "test_accuracy.append(precision_micro)\n",
        "test_accuracy.append(precision_macro)\n",
        "test_accuracy.append(f1)\n",
        "np.save(save_path + '/test_accuracy', test_accuracy)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 26\n",
            "Precision Micro 0.9863275192762194\n",
            "Precision Macro 0.9339168715378928\n",
            "F1 Score        0.9295148147428725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ryErls8p_nr",
        "colab_type": "code",
        "outputId": "b7f3793c-f612-437c-e4f7-385106db3140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "\n",
        "df_cm = pd.DataFrame(con_matrix, index = [i for i in ['ORG', 'PER', 'LOC', 'O']],\n",
        "                  columns = [i for i in ['ORG', 'PER', 'LOC', 'O']])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig(save_path+'/confusion_matrix')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}